<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-22T21:47:50-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Robot psychologist‚Äôs blog</title><subtitle>Welcome to Robot Psychologist&apos;s Blog ‚Äî a blog where I unpack the inner workings of intelligent systems, one curious question at a time.  I&apos;m a PhD student in AI with a BSc in neuroscience and psychology, and this is my space to explore the ideas. If you&apos;re into how intelligent systems (human or artificial) learn, adapt, and act, you&apos;ll feel right at home.
</subtitle><author><name>Busra Tugce Gurbuz</name></author><entry><title type="html">Stop Letting Your GPU Nap: Stack Jobs and Supercharge Your Experiments</title><link href="http://localhost:4000/compute/2024/07/22/GPUs.html" rel="alternate" type="text/html" title="Stop Letting Your GPU Nap: Stack Jobs and Supercharge Your Experiments" /><published>2024-07-22T00:00:00-04:00</published><updated>2024-07-22T00:00:00-04:00</updated><id>http://localhost:4000/compute/2024/07/22/GPUs</id><content type="html" xml:base="http://localhost:4000/compute/2024/07/22/GPUs.html"><![CDATA[<p><em>Tips for ML researchers on shared clusters who are tired of slow experiments and sleepy GPUs.</em></p>

<hr />

<h3 id="wait-why-is-my-gpu-so-bored-">Wait, Why Is My GPU So Bored? ü•π</h3>

<p>Ever peeked at <code class="language-plaintext highlighter-rouge">nvidia-smi</code> mid-training and felt personally offended by a <strong>15% GPU utilization</strong> reading?</p>

<p>You‚Äôre not alone.</p>

<p>In many ML setups‚Äîespecially in deep reinforcement learning or self-supervised learning‚Äîthe GPU ends up spending more time <strong>waiting around</strong> than doing actual work. Here‚Äôs why:</p>

<ul>
  <li>Your model might be <strong>tiny</strong> (looking at you, MLPs and small CNNs).</li>
  <li><strong>Environment steps</strong> in RL live on the CPU and take their sweet time.</li>
  <li><strong>Data augmentation</strong> and preprocessing often clog the CPU while the GPU twiddles its thumbs.</li>
  <li>Even classic vision or SimCLR jobs on CIFAR-10 barely dent the surface of a modern A100‚Äôs power.</li>
</ul>

<p>Moral of the story? <strong>You‚Äôve got untapped compute just sitting there.</strong></p>

<h3 id="signs-of-gpu-underuse">Signs of GPU Underuse</h3>

<p>Here‚Äôs how to know your GPU‚Äôs taking a nap:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">nvidia-smi</code> shows <strong>plenty of free VRAM</strong> (e.g., using 5 GB out of 40 GB).</li>
  <li>Compute ‚ÄúUtil‚Äù column idles in the teens while the CPU sits near 100 %.
    <ul>
      <li>Example: a fastai ResNet-18 computer-vision run on an A100 sat at ~20 % util with memory to spare (<a href="https://stackoverflow.com/questions/75553862/low-utilization-of-the-a100-gpu-with-fastai">reference</a>) or an RLlib DQN job with 256 k batch size still spiked only briefly above 25 %</li>
    </ul>
  </li>
</ul>

<p>You might be tempted to buy more GPUs. Don‚Äôt. <strong>Use what you already have better.</strong></p>

<h3 id="the-secret-run-multiple-jobs-at-once">The Secret: Run Multiple Jobs at Once</h3>

<p>If your current job is only using a slice of the GPU, just stack more on top!</p>

<p>Here‚Äôs the magic formula:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run three jobs in parallel</span>
<span class="k">for </span>seed <span class="k">in </span>0 1 2<span class="p">;</span> <span class="k">do
    </span>python train.py <span class="nt">--seed</span> <span class="nv">$seed</span> &amp;   <span class="c"># Launch in background</span>
<span class="k">done
</span><span class="nb">wait</span>  <span class="c"># Let them all finish before exiting</span>
</code></pre></div></div>

<p>Why it works:</p>

<ul>
  <li>
    <p>Each job uses a slice of VRAM; their peaks rarely coincide.</p>
  </li>
  <li>Streaming Multiprocessor stay busier because when one job waits on the CPU, another is mid-backprop.
    <ul>
      <li><strong>More info on SMs:</strong> Each SM handles the actual math operations (like matrix multiplies and convolutions). A100 has 108 SMs, which means it can handle a lot of parallel math ‚Äî if you feed it well.</li>
    </ul>
  </li>
  <li>You triple sweep throughput without touching the cluster queue.</li>
</ul>

<h3 id="slurm-friendly-version-yes-it-works-on-clusters">SLURM-Friendly Version (Yes, It Works on Clusters)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --job-name=stacked_runs</span>
<span class="c">#SBATCH --gres=gpu:a100:1</span>
<span class="c">#SBATCH --cpus-per-task=8</span>
<span class="c">#SBATCH --mem=32G</span>

module load anaconda/3
conda activate ml-env
<span class="nb">cd</span> <span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c"># Launch 3 jobs concurrently</span>
<span class="k">for </span>cfg <span class="k">in </span>cfg1.yaml cfg2.yaml cfg3.yaml<span class="p">;</span> <span class="k">do
    </span>python train.py <span class="nt">--config</span> <span class="nv">$cfg</span> &amp; 
<span class="k">done
</span><span class="nb">wait</span>
</code></pre></div></div>

<p>This trick works great for:</p>

<ul>
  <li>Hyperparameter sweeps</li>
  <li>Seed averaging</li>
  <li>Trying three ideas because you‚Äôre impatient (relatable)</li>
</ul>

<h3 id="tips-pitfalls-and-gotchas-with-explanations">Tips, Pitfalls, and Gotchas (With Explanations!)</h3>

<table>
  <thead>
    <tr>
      <th>‚úÖ / ‚ö†Ô∏è</th>
      <th>What You Should Know</th>
      <th>Why It Matters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>‚úÖ</td>
      <td><strong>Leave ~10% VRAM unused</strong></td>
      <td>PyTorch loves to surprise you with memory spikes. A small buffer helps you avoid sudden OOM crashes that wipe out <em>all</em> jobs.</td>
    </tr>
    <tr>
      <td>‚úÖ</td>
      <td><strong>Use <code class="language-plaintext highlighter-rouge">/scratch</code> or SSD storage</strong></td>
      <td>If three jobs all hit the disk at once on slow storage, your fancy parallelism will turn into a data-loading traffic jam.</td>
    </tr>
    <tr>
      <td>‚úÖ</td>
      <td><strong>Tag runs in your logger (e.g., <code class="language-plaintext highlighter-rouge">wandb --group stacked</code>)</strong></td>
      <td>Keeps your dashboards from looking like a spaghetti bowl of metrics. Easier to compare, track, and brag about.</td>
    </tr>
    <tr>
      <td>‚úÖ</td>
      <td><strong>Watch <code class="language-plaintext highlighter-rouge">num_workers</code> and threads</strong></td>
      <td>Each job spawns data loaders. Multiply that by three and suddenly your system has 48 zombie processes hoarding RAM. Keep things lean.</td>
    </tr>
    <tr>
      <td>‚ö†Ô∏è</td>
      <td><strong>Don‚Äôt stack giant models</strong></td>
      <td>If you‚Äôre running LLMs, ViTs, or anything eating 80%+ VRAM, just‚Ä¶ don‚Äôt. You‚Äôll get out-of-memory errors faster than you can say ‚ÄúSIGKILL‚Äù.</td>
    </tr>
    <tr>
      <td>‚ö†Ô∏è</td>
      <td><strong>Know your cluster‚Äôs rules</strong></td>
      <td>Some clusters have strict policies: one job per GPU, no background processes, etc. Break them, and you might lose access. Nobody wants that email.</td>
    </tr>
  </tbody>
</table>

<h3 id="tldr-">TL;DR üíõ</h3>

<p><strong>If your GPU looks bored, it probably is.</strong></p>

<p>Instead of leaving it idle, stack 2‚Äì3 light-to-medium jobs on the same card. You‚Äôll:</p>

<ul>
  <li>Finish sweeps 2‚Äì3x faster</li>
  <li>Reduce total GPU-hours</li>
  <li>Help your labmates get off the waitlist</li>
</ul>

<h3 id="your-move-">Your Move üíÖ</h3>

<ol>
  <li>Fire up few extra jobs.</li>
  <li>Monitor <code class="language-plaintext highlighter-rouge">nvidia-smi</code>.</li>
  <li>Watch your GPU actually break a sweat.</li>
  <li>Flex your productivity gains.</li>
</ol>

<p>You don‚Äôt need more compute‚Äîyou just need to <strong>use it smarter</strong>.</p>]]></content><author><name>Busra Tugce Gurbuz</name></author><category term="compute" /><summary type="html"><![CDATA[Tips for ML researchers on shared clusters who are tired of slow experiments and sleepy GPUs.]]></summary></entry></feed>