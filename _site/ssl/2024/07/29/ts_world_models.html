<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>From BYOL to JEPA: How Student–Teacher Networks Quietly Became the Brains Behind World Models | Robot psychologist’s blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="From BYOL to JEPA: How Student–Teacher Networks Quietly Became the Brains Behind World Models" />
<meta name="author" content="Busra Tugce Gurbuz" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Training an agent that can reason about its environment—without ground‑truth labels—requires a robust internal simulator, often called a world model. Among the many ideas in self‑supervised learning (SSL), the student–teacher paradigm has proven repeatedly effective at producing the high‑quality representations that such world models depend on." />
<meta property="og:description" content="Training an agent that can reason about its environment—without ground‑truth labels—requires a robust internal simulator, often called a world model. Among the many ideas in self‑supervised learning (SSL), the student–teacher paradigm has proven repeatedly effective at producing the high‑quality representations that such world models depend on." />
<link rel="canonical" href="http://localhost:4000/ssl/2024/07/29/ts_world_models.html" />
<meta property="og:url" content="http://localhost:4000/ssl/2024/07/29/ts_world_models.html" />
<meta property="og:site_name" content="Robot psychologist’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-29T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="From BYOL to JEPA: How Student–Teacher Networks Quietly Became the Brains Behind World Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Busra Tugce Gurbuz"},"dateModified":"2024-07-29T00:00:00-04:00","datePublished":"2024-07-29T00:00:00-04:00","description":"Training an agent that can reason about its environment—without ground‑truth labels—requires a robust internal simulator, often called a world model. Among the many ideas in self‑supervised learning (SSL), the student–teacher paradigm has proven repeatedly effective at producing the high‑quality representations that such world models depend on.","headline":"From BYOL to JEPA: How Student–Teacher Networks Quietly Became the Brains Behind World Models","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ssl/2024/07/29/ts_world_models.html"},"url":"http://localhost:4000/ssl/2024/07/29/ts_world_models.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Robot psychologist&apos;s blog" />
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Robot psychologist&#39;s blog</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon fas fa-bars fa-lg"></span>
        </label>

        <div class="drawer-container">
          <div class="drawer">
  <a class="nav-item" href="/about/">About</a>
          </div>
        </div>
        <div class="slab">
  <a class="nav-item" href="/about/">About</a>
        </div>
      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">From BYOL to JEPA: How Student–Teacher Networks Quietly Became the Brains Behind World Models</h1>
    <div class="post-meta">
      <time class="dt-published" datetime="2024-07-29T00:00:00-04:00" itemprop="datePublished">
        Jul 29, 2024
      </time>
    </div>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Training an agent that can reason about its environment—without ground‑truth labels—requires a robust internal simulator, often called a <strong>world model</strong>.  Among the many ideas in self‑supervised learning (SSL), the <strong>student–teacher paradigm</strong> has proven repeatedly effective at producing the high‑quality representations that such world models depend on.</p>

<p>This post explains how student–teacher SSL works, why it has been influential and might be important for building better world models.</p>

<h3 id="what-is-a-studentteacher-network">What Is a Student–Teacher Network?</h3>

<p>A student–teacher setup contains two networks:</p>

<ul>
  <li><strong>Teacher</strong> – Provides target features or predictions.</li>
  <li><strong>Student</strong> – Trains to match those targets across augmented views of the same input.</li>
</ul>

<p>The teacher is usually an <strong>exponential moving average (EMA)</strong> of the student, so its parameters evolve slowly and provide a stable learning signal.  Because targets come from the model itself rather than external labels, the approach scales to unlabeled data.</p>

<h3 id="why-it-works">Why It Works</h3>

<ul>
  <li><strong>Soft targets carry richer information</strong> than one‑hot labels, exposing inter‑class structure and uncertainty.</li>
  <li><strong>Temporal smoothing</strong> via EMA aggregates knowledge over many updates, acting like an implicit ensemble.</li>
  <li><strong>Augmentation consistency</strong> forces invariance to viewpoint, color, cropping and other real‑world nuisance factors.</li>
</ul>

<h3 id="a-brief-historical-detour">A Brief Historical Detour</h3>

<p>The student–teacher idea is far from new; it has surfaced in diverse corners of SSL for nearly a decade.</p>

<ul>
  <li><strong>Temporal Ensembling (2017)</strong> – Improved semi‑supervised classification by averaging model predictions over multiple epochs.</li>
  <li><strong>Mean Teacher (2017)</strong> – Used EMA weights explicitly to create a teacher for consistency regularisation.</li>
  <li><strong>MoCo (2019)</strong> – Employed a momentum encoder (teacher) to populate a memory bank for contrastive learning.</li>
  <li><strong>BYOL (2020)</strong> – Demonstrated that a student can learn useful features from an EMA teacher without any negative pairs.</li>
  <li><strong>DINO (2021)</strong> – Showed that applying centering and sharpening to teacher outputs prevents collapse at scale.</li>
  <li><strong>JEPA family (2023 – present)</strong> – Recasts the student–teacher idea as <strong>masked‑region prediction</strong>: the student, given only a partial view, predicts the teacher’s features for the hidden region.  This simple shift from view alignment to spatial prediction yields object‑centric, forward‑looking representations—ideal for world‑model objectives.</li>
  <li><strong>Speech, NLP and Multimodal Work</strong> – Similar momentum‑distillation ideas power HuBERT, CLIP variants, and more.</li>
</ul>

<h4 id="why-this-lineage-matters">Why This Lineage Matters</h4>

<ol>
  <li><strong>Evidence of robustness</strong> – The same principle succeeds across vision, speech and language, suggesting a fundamental mechanism.</li>
  <li><strong>Design inspiration</strong> – Historical tricks (memory banks, centering, sharpening, predictor asymmetry) offer a toolbox for future models.</li>
  <li><strong>Theoretical grounding</strong> – Understanding how targets evolve sheds light on why collapse happens and how to avoid it.</li>
  <li><strong>Transferable intuition</strong> – Insights gained in vision often translate to other modalities, accelerating cross‑domain progress.</li>
</ol>

<h3 id="moving-beyond-contrastive-learning">Moving Beyond Contrastive Learning</h3>

<p>Early SSL methods such as SimCLR and InfoNCE relied on negative pairs: anchor‑positive similarities were maximised while anchor‑negative similarities were minimised.  This led to:</p>

<ul>
  <li>Large batch‑size requirements.</li>
  <li>Risk of <strong>false negatives</strong> when semantically similar images were pushed apart.</li>
  <li>Additional engineering (memory banks, distributed synchronisation).</li>
</ul>

<p>Student–teacher methods sidestep these issues by <strong>removing negatives altogether</strong>.  Instead, learning is driven by matching the teacher’s targets, greatly simplifying training and improving stability.</p>

<h3 id="collapse-and-how-to-avoid-it">Collapse and How to Avoid It</h3>

<p>Removing negatives introduces a new risk: the student may converge to a trivial solution where every input maps to the same embedding, a phenomenon known as “collapse”.</p>

<ul>
  <li><strong>BYOL</strong> counters this with architectural asymmetry (student has an extra predictor) and EMA updates.</li>
  <li><strong>DINO</strong> keeps both networks identical but applies two regularisers:
    <ul>
      <li><strong>Centering</strong> subtracts a running mean to prevent feature domination.</li>
      <li><strong>Sharpening</strong> uses low‑temperature softmax to encourage confident, non‑uniform outputs.</li>
    </ul>
  </li>
</ul>

<p>These tricks maintain diversity without re‑introducing negatives.</p>

<h3 id="why-studentteacher-ssl-is-good-for-world-models">Why Student–Teacher SSL Is Good for World Models</h3>

<p>World models could require learning from vast streams of partially observed, noisy data—exactly where labels are hardest to obtain.  Student–teacher SSL provides:</p>

<ul>
  <li><strong>Label‑free scalability</strong> – Works on billions of internet images, raw video, agent rollouts or multimodal corpora.</li>
  <li><strong>Stability</strong> – EMA teachers and regularisation avoid collapse and noisy gradients.</li>
  <li><strong>Computational efficiency</strong> – No need for giant batches or external memory structures.</li>
  <li><strong>Modality agnosticism</strong> – Proven in vision, speech and language, making it ideal for unified, multi‑sensor world models.</li>
</ul>

<p>Consider JEPA as a concrete variation: rather than aligning two augmented views of the same input, it trains the student to predict the teacher’s representation of a masked region using only partial context. This predictive setup still relies on a student–teacher framework (with an EMA teacher), but shifts the objective toward inferring latent structure.  Variants of this idea could lead to increasingly powerful and scalable pretraining strategies for world models.</p>

<h3 id="key-takeaways">Key Takeaways</h3>

<ol>
  <li>
    <p><strong>Proven across tasks and modalities</strong><br />
Student–teacher SSL has driven breakthroughs from Mean Teacher and MoCo to BYOL, DINO, and the JEPA family, as well as in speech (HuBERT) and multimodal settings (CLIP variants). Its repeated success points to a broadly applicable learning principle.</p>
  </li>
  <li>
    <p><strong>Evolving design space</strong><br />
Classic tricks—memory banks, centering, sharpening, predictor asymmetry—remain useful, while newer predictive variants such as JEPA demonstrate that simply changing <em>what</em> the student predicts (alignment vs. masked‑region inference) can open entirely new capabilities.</p>
  </li>
  <li>
    <p><strong>Simpler and more compute‑friendly than contrastive methods</strong><br />
By eliminating negative pairs and large batch requirements, student–teacher approaches reduce engineering overhead and make large‑scale pretraining more accessible.</p>
  </li>
  <li>
    <p><strong>Well matched to world‑model objectives</strong><br />
The consistency‑based signals that guide student–teacher SSL align naturally with the need to infer hidden or future state. JEPA’s masked‑region prediction shows how the same machinery can be adapted for forward‑simulation tasks.</p>
  </li>
  <li>
    <p><strong>Valuable, but not the only tool</strong><br />
A strong grasp of student–teacher SSL provides a practical head start for building richer world models, yet it can be complemented with other techniques—contrastive objectives, generative modeling, or reinforcement learning—to meet specific domain requirements.</p>
  </li>
</ol>

  </div><a class="u-url" href="/ssl/2024/07/29/ts_world_models.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
              />
            </svg><span>Subscribe</span>
          </a>
        </p> -->
        <ul class="contact-list">
          <li class="p-name">Busra Tugce Gurbuz</li>
          <li><a class="u-email" href="mailto:"></a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Welcome to Robot Psychologist&#39;s Blog — a blog where I unpack the inner workings of intelligent systems, one curious question at a time.  I&#39;m a PhD student in AI with a BSc in neuroscience and psychology, and this is my space to explore the ideas. If you&#39;re into how intelligent systems (human or artificial) learn, adapt, and act, you&#39;ll feel right at home.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>

</body>

</html>
